action_repeat: 2
episode_length: 100/${action_repeat}
train_steps: 200000/${action_repeat}
min_std: 0.05

iterations: 6
num_samples: 700 # 256 for icem, 512 default
num_elites: 32

goal_dim: 0
her: false

seed: 0
wandb_exp_name: default
noise_beta: 0.5
rho: 0.5
td_lambda: 0.2
train_interval: 5
intrinsic_reward_coef: 0.25
mixture_coef: 0.5
init_std: 0.5

latent_dim: 100
horizon: 6
std_schedule: linear(0.5, ${min_std}, 5000, 1000)  # duration 25000 train step
horizon_schedule: linear(2, ${horizon}, 5000, 1000)
explore_schedule: linear(0, ${intrinsic_reward_coef}, 1000, 1000)
regularization_schedule: linear(0.05, ${mixture_coef}, 1, 2000)

# reset q
reset_q: true
reset_interval: 500
q_tau: 0.8
q_lr: 1e-3
alpha_bc: 0.0

save_interval: 200 # count in epoch
eval_freq_episodes: 25
seed_steps: 1000